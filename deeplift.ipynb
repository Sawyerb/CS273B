{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d5b9c6f81cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cs273b_params.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparams_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_open\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparams_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparams_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "with open('cs273b_params.json') as params_open:\n",
    "    params = json.load(params_open)\n",
    "params_model = params['model']\n",
    "params_train = params['train']\n",
    "\n",
    "model = seqnn.SeqNN(params_model)\n",
    "model.model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import deeplift\n",
    "from deeplift.conversion import kerasapi_conversion as kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "small_train_input = np.random.randint(1, 4, size=(10000, 1000, 4))\n",
    "small_train_output = np.zeros((10000, 4))\n",
    "print(len(small_train_input))\n",
    "for i in range(len(small_train_input)):\n",
    "    small_train_input[i:,0:20,:] = i%2\n",
    "    small_train_output[i,:] = i%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'leftpos': 9500, 'rightpos':10500,\n",
    "#           'numconvlayers': {'numFiltersConv1': 128, 'numFiltersConv2': 32,  'filterLenConv1':6, 'filterLenConv2': 9},\n",
    "#          'dilRate1': 1,\n",
    "#          'maxPool1': 30, 'dilRate2': 1, 'maxPool2': 10,\n",
    "#          'numdenselayers': {'dense1': 64, 'dense2': 2, 'dropout1':0.00099, 'dropout2':0.01546},\n",
    "#          'activationFxn': 'sigmoid',\n",
    "#          'batch_size': 128}\n",
    "#params = {'leftpos': 9500, 'rightpos':10500,\n",
    "#          'numconvlayers': {'numFiltersConv1': 128, 'numFiltersConv2': 32,  'filterLenConv1':6, 'filterLenConv2': 9},\n",
    "#         'dilRate1': 1,\n",
    "#         'maxPool1': 30, 'dilRate2': 1, 'maxPool2': 10,\n",
    "#         'numdenselayers': {'dense1': 64, 'dense2': 16, 'dropout1':0.00099, 'dropout2':0.01546},\n",
    "#         'activationFxn': 'relu',\n",
    "#         'batch_size': 128}\n",
    "params = {'leftpos': 9500, 'rightpos':10500,\n",
    "          'numconvlayers': {'numFiltersConv1': 128, 'numFiltersConv2': 144, 'numFiltersConv3': 162, 'numFiltersConv4': 202, 'numFiltersConv5': 253, 'filterLenConv1':17, 'filterLenConv2': 9, 'filterLenConv3-5': 5},\n",
    "         'dilRate1': 1,\n",
    "         'maxPool1': 5, 'dilRate2': 1, 'maxPool2': 5, 'maxPool3-5': 2, 'dilRate3-5': 1,\n",
    "         'numdenselayers': {'dense1': 64, 'dense2': 16, 'dropout1':0.4, 'dropout2':0.3},\n",
    "         'activationFxn': 'relu',\n",
    "         'batch_size': 256}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # leftpos = int(params['leftpos'])\n",
    "# # rightpos = int(params['rightpos'])\n",
    "# # X_trainpromoterSubseq = X_trainpromoter[:,leftpos:rightpos,:]\n",
    "# # X_validpromoterSubseq = X_validpromoter[:,leftpos:rightpos,:]\n",
    "# # input_promoter = Input(shape=X_trainpromoterSubseq.shape[1:], name='promoter')\n",
    "# learning_rates = [0.0001]\n",
    "\n",
    "# for lr in learning_rates:\n",
    "#     input_promoter = Input(shape=small_train_input.shape[1:], name='promoter')\n",
    "\n",
    "#     x = Conv1D(int(params['numconvlayers']['numFiltersConv1']), int(params['numconvlayers']['filterLenConv1']), dilation_rate=int(params['dilRate1']), padding='same', kernel_initializer='glorot_normal', input_shape=small_train_input.shape[1:],activation=params['activationFxn'])(input_promoter)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling1D(int(params['maxPool1']))(x)\n",
    "#     x = Conv1D(int(params['numconvlayers']['numFiltersConv2']), int(params['numconvlayers']['filterLenConv2']), dilation_rate=int(params['dilRate2']), padding='same', kernel_initializer='glorot_normal',activation=params['activationFxn'])(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling1D(params['maxPool2'])(x)\n",
    "#     x = Conv1D(int(params['numconvlayers']['numFiltersConv3']), int(params['numconvlayers']['filterLenConv3-5']), dilation_rate=int(params['dilRate3-5']), padding='same', kernel_initializer='glorot_normal',activation=params['activationFxn'])(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling1D(params['maxPool3-5'])(x)\n",
    "#     x = Conv1D(int(params['numconvlayers']['numFiltersConv4']), int(params['numconvlayers']['filterLenConv3-5']), dilation_rate=int(params['dilRate3-5']), padding='same', kernel_initializer='glorot_normal',activation=params['activationFxn'])(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling1D(params['maxPool3-5'])(x)\n",
    "#     x = Conv1D(int(params['numconvlayers']['numFiltersConv5']), int(params['numconvlayers']['filterLenConv3-5']), dilation_rate=int(params['dilRate3-5']), padding='same', kernel_initializer='glorot_normal',activation=params['activationFxn'])(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling1D(params['maxPool3-5'])(x)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(int(params['numdenselayers']['dense1']))(x)\n",
    "#     x = Activation(params['activationFxn'])(x)\n",
    "#     x = Dropout(params['numdenselayers']['dropout1'])(x)\n",
    "#     #x = Dense(int(params['numdenselayers']['dense2']))(x)\n",
    "#     #x = Activation(params['activationFxn'])(x)\n",
    "#     #x = Dropout(params['numdenselayers']['dropout2'])(x)\n",
    "#     x = Dense(4)(x)\n",
    "#     main_output = Activation('softplus')(x) # force outputs to be at least 0\n",
    "#     model = Model(inputs=[input_promoter], outputs=[main_output])\n",
    "#     model.compile(Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),'poisson', metrics=['mean_squared_error'])\n",
    "#     model.summary()\n",
    "    \n",
    "#     model.fit(x=small_train_input, y=small_train_output,epochs=10, shuffle='batch',\n",
    "#               batch_size=params['batch_size'])\n",
    "#     model.save('test_deeplift_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/src/deepexplain/deepexplain/tensorflow/methods.py:556: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepexplain.tensorflow import DeepExplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticShift(tf.keras.layers.Layer):\n",
    "  \"\"\"Stochastically shift a one hot encoded DNA sequence.\"\"\"\n",
    "  def __init__(self, shift_max=0, pad='uniform', **kwargs):\n",
    "    super(StochasticShift, self).__init__(**kwargs)\n",
    "    self.augment_shifts = tf.range(-shift_max, shift_max+1)\n",
    "    self.pad = pad\n",
    "  def call(self, seq_1hot, training):\n",
    "    def stoch_shift():\n",
    "      shift_i = tf.random.uniform(shape=[], minval=0,\n",
    "        maxval=len(self.augment_shifts), dtype=tf.int64)\n",
    "      shift = tf.gather(self.augment_shifts, shift_i)\n",
    "\n",
    "      sseq_1hot = tf.cond(tf.not_equal(shift, 0),\n",
    "                          lambda: shift_sequence(seq_1hot, shift),\n",
    "                          lambda: seq_1hot)\n",
    "      return sseq_1hot\n",
    "\n",
    "    return tf.cond(training,\n",
    "                   stoch_shift,\n",
    "                   lambda: seq_1hot)\n",
    "\n",
    "class GELU(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GELU, self).__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        return tf.keras.activations.sigmoid(1.702 * x) * x\n",
    "    \n",
    "def shift_sequence(seq, shift, pad_value=0.25):\n",
    "  \"\"\"Shift a sequence left or right by shift_amount.\n",
    "\n",
    "  Args:\n",
    "  seq: [batch_size, seq_length, seq_depth] sequence\n",
    "  shift: signed shift value (tf.int32 or int)\n",
    "  pad_value: value to fill the padding (primitive or scalar tf.Tensor)\n",
    "  \"\"\"\n",
    "  if seq.shape.ndims != 3:\n",
    "      raise ValueError('input sequence should be rank 3')\n",
    "  input_shape = seq.shape\n",
    "\n",
    "  pad = pad_value * tf.ones_like(seq[:, 0:tf.abs(shift), :])\n",
    "\n",
    "  def _shift_right(_seq):\n",
    "    # shift is positive\n",
    "    sliced_seq = _seq[:, :-shift:, :]\n",
    "    return tf.concat([pad, sliced_seq], axis=1)\n",
    "\n",
    "  def _shift_left(_seq):\n",
    "    # shift is negative\n",
    "    sliced_seq = _seq[:, -shift:, :]\n",
    "    return tf.concat([sliced_seq, pad], axis=1)\n",
    "\n",
    "  sseq = tf.cond(tf.greater(shift, 0),\n",
    "                 lambda: _shift_right(seq),\n",
    "                 lambda: _shift_left(seq))\n",
    "  sseq.set_shape(input_shape)\n",
    "\n",
    "  return sseq\n",
    "\n",
    "class PearsonR(tf.keras.metrics.Metric):\n",
    "  def __init__(self, num_targets, summarize=True, name='pearsonr', **kwargs):\n",
    "    super(PearsonR, self).__init__(name=name, **kwargs)\n",
    "    self._summarize = summarize\n",
    "    self._shape = (num_targets,)\n",
    "    self._count = self.add_weight(name='count', shape=self._shape, initializer='zeros')\n",
    "\n",
    "    self._product = self.add_weight(name='product', shape=self._shape, initializer='zeros')\n",
    "    self._true_sum = self.add_weight(name='true_sum', shape=self._shape, initializer='zeros')\n",
    "    self._true_sumsq = self.add_weight(name='true_sumsq', shape=self._shape, initializer='zeros')\n",
    "    self._pred_sum = self.add_weight(name='pred_sum', shape=self._shape, initializer='zeros')\n",
    "    self._pred_sumsq = self.add_weight(name='pred_sumsq', shape=self._shape, initializer='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequence (InputLayer)        [(None, 1000, 4)]         0         \n",
      "_________________________________________________________________\n",
      "stochastic_shift (Stochastic (None, 1000, 4)           0         \n",
      "_________________________________________________________________\n",
      "gelu (GELU)                  (None, 1000, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1000, 128)         8704      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1000, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "gelu_1 (GELU)                (None, 200, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 144)          165888    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 144)          576       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 40, 144)           0         \n",
      "_________________________________________________________________\n",
      "gelu_2 (GELU)                (None, 40, 144)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 40, 162)           116640    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 40, 162)           648       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 20, 162)           0         \n",
      "_________________________________________________________________\n",
      "gelu_3 (GELU)                (None, 20, 162)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 20, 182)           147420    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 20, 182)           728       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 182)           0         \n",
      "_________________________________________________________________\n",
      "gelu_4 (GELU)                (None, 10, 182)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 10, 205)           186550    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 10, 205)           820       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 5, 205)            0         \n",
      "_________________________________________________________________\n",
      "gelu_5 (GELU)                (None, 5, 205)            0         \n",
      "_________________________________________________________________\n",
      "gelu_6 (GELU)                (None, 5, 205)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5, 32)             32800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 5, 32)             128       \n",
      "_________________________________________________________________\n",
      "cropping1d (Cropping1D)      (None, 1, 32)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1, 4)              132       \n",
      "=================================================================\n",
      "Total params: 661,546\n",
      "Trainable params: 659,840\n",
      "Non-trainable params: 1,706\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# #model = keras.models.load_model('model.h5')\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from basenji import seqnn\n",
    "# import json\n",
    "# model = tf.keras.models.load_model('model.h5', custom_objects={'StochasticShift': StochasticShift, 'GELU': GELU, \n",
    "#                                                             'shift_sequence':shift_sequence, 'PearsonR': PearsonR})\n",
    "with open('cs273b_params.json') as params_open:\n",
    "    params = json.load(params_open)\n",
    "params_model = params['model']\n",
    "params_train = params['train']\n",
    "\n",
    "model = seqnn.SeqNN(params_model)\n",
    "model.model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The model output must be a vector or a single value!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c57d3972cd45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmall_train_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_train_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_train_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/shap/explainers/deep/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_phase_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/shap/explainers/deep/deep_tf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is not currently a supported model type!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The model output to be explained must be a single tensor!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The model output must be a vector or a single value!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The model output must be a vector or a single value!"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "background = small_train_input[np.random.choice(small_train_input.shape[0], 100, replace=False)]\n",
    "e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(small_train_input[200:201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`get_session` is not available when TensorFlow is executing eagerly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5b91f1803d6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mDeepExplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mde\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         raise RuntimeError(\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;34m'`get_session` is not available when '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m             'TensorFlow is executing eagerly.')\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `get_session` is not available when TensorFlow is executing eagerly."
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=keras.backend.get_session()) as de:\n",
    "    input_tensor = model.layers[0].input\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    xs = small_train_input[0:10]\n",
    "    ys = small_train_input[0:10]\n",
    "    attributions_gradin = de.explain('grad*input', target_tensor, input_tensor, xs, ys=ys)\n",
    "    print(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap`\n",
    "background = small_train_input[np.random.choice(small_train_input.shape[0], 100, replace=False)]\n",
    "e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(small_train_input[200:205])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplift_model =\\\n",
    "    kc.convert_model_from_saved_files(\n",
    "        'test_deeplift_model.h5',\n",
    "        nonlinear_mxts_mode=deeplift.layers.NonlinearMxtsMode.DeepLIFT_GenomicsDefault) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_output.shape)\n",
    "print(np.argwhere(test_output > 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_input[651910:651910+3]\n",
    "print(t)\n",
    "t.shape\n",
    "print(model.predict(t))\n",
    "print(test_output[651910:651910+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input[36576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_output[:20000,0], model.predict(test_input[:20000])[:,0])\n",
    "plt.xlabel('Real Activation for Cell Type 0')\n",
    "plt.ylabel('Predicted Activation for Cell Type 0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_class = 2 # the index of the output class we want to maximize\n",
    "output = model.layers[-1].output\n",
    "target = tf.math.scalar_mul(2, output[:,output_class])\n",
    "m = tf.math.reduce_mean(output, axis=1)\n",
    "loss = m - target\n",
    "grads = tf.gradients(loss, model.input)[0] # the output of `gradients` is a list, just take the first (and only) element\n",
    "iterate = keras.backend.function([model.input], [loss, grads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq():\n",
    "    def adam_update(grad, i, w):\n",
    "        #initialize the values of the parameters\n",
    "        alpha = 1\n",
    "        beta_1 = 0.9\n",
    "        beta_2 = 0.999 \n",
    "        epsilon = 1e-8\n",
    "\n",
    "        nonlocal theta_0\n",
    "        nonlocal m\n",
    "        nonlocal v \n",
    "\n",
    "        m = beta_1 * m + (1 - beta_1) * grad\n",
    "        v = beta_2 * v + (1 - beta_2) * np.power(grad, 2)\n",
    "        m_hat = m / (1 - np.power(beta_1, i))\n",
    "        v_hat = v / (1 - np.power(beta_2, i))\n",
    "        w = w + alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        return w\n",
    "\n",
    "    theta_0 = 0\n",
    "    m = 0 \n",
    "    v = 0 \n",
    "    \n",
    "    raw_input_seq = np.random.random((1, 1000, 4)) # define an initial random image\n",
    "\n",
    "    lr = 1.  # learning rate used for gradient updates\n",
    "    max_iter = 1000  # number of gradient updates iterations\n",
    "    for i in range(max_iter):\n",
    "        loss_val, grads_val = iterate([raw_input_seq])\n",
    "        #print(grads_val)\n",
    "        raw_input_seq = adam_update(grads_val, i+1, raw_input_seq)\n",
    "        \n",
    "        #raw_input_seq += grads_val * lr  # update the image based on gradients\n",
    "        if i % 100 == 0:\n",
    "            pred = model.predict(raw_input_seq)\n",
    "            print('Iteration ' + str(i) + ': model activation ' + str(pred))\n",
    "    \n",
    "    # Convert back to one-hot encoding\n",
    "    raw_input_seq = np.squeeze(raw_input_seq)\n",
    "    max_activations = np.argmax(raw_input_seq, axis=1).flatten()\n",
    "    encoded_seq = np.zeros(raw_input_seq.shape)\n",
    "    encoded_seq[np.arange(encoded_seq.shape[0]), max_activations] = 1\n",
    "    \n",
    "    return raw_input_seq, encoded_seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    input_seq, encoded_seq = generate_seq()\n",
    "    print(input_seq[:10,])\n",
    "    print(encoded_seq[:10,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
